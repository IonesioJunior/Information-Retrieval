{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "\n",
    "__author__ = \"Ionésio Junior\"\n",
    "\n",
    "# NLTK imports\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Prepare our dataset </h3>\n",
    "Here, let's read and merge title, subtitle, and content from our documents in just one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file  using utf-8 format\n",
    "news = pd.read_csv(\"estadao_noticias_eleicao.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Merge title, subtitle and content columns\n",
    "content = news.titulo + \" \" + news.subTitulo + \" \" + news.conteudo\n",
    "content = content.fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Filtering dataset</h3>\n",
    "Now we are going to filter some noise in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dots\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_lists = content.apply(lambda text: tokenizer.tokenize(text.lower()))\n",
    "\n",
    "#Remove portuguese stop words\n",
    "stopword_ = stopwords.words('portuguese')\n",
    "filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build co-occurrence matrix( First step )</h3>\n",
    "Here we are going to build our co-occurrence words matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    n = len(vocab)\n",
    "   \n",
    "    vocab_to_index = {word:i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    bi_grams = list(bigrams(corpus))\n",
    "\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "\n",
    "    I=list()\n",
    "    J=list()\n",
    "    V=list()\n",
    "    \n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "\n",
    "        I.append(vocab_to_index[previous])\n",
    "        J.append(vocab_to_index[current])\n",
    "        V.append(count)\n",
    "        \n",
    "    co_occurrence_matrix = sparse.coo_matrix((V,(I,J)), shape=(n,n))\n",
    "\n",
    "    return co_occurrence_matrix, vocab_to_index\n",
    "\n",
    "\n",
    "#Transforming list of lists into one list\n",
    "tokens = [token for tokens_list in filtered_tokens for token in tokens_list]\n",
    "matrix, vocab = co_occurrence_matrix(tokens)\n",
    "consultable_matrix = matrix.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Co-occurency score words( Second Step )</h3>\n",
    "Get most co-ocurrency score words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_words_co_ocurrency(word,k=3):\n",
    "    results = consultable_matrix[vocab[word]].toarray()[0]\n",
    "    indexes, L_sorted = zip(*sorted(enumerate(results), key=itemgetter(1),reverse=True))\n",
    "    return [ [ x for x in vocab.keys() if vocab[x] == y ][0] for y in indexes[:k] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Previous implementation of inverted index search</h2>\n",
    "We will use a simple inverted index search to compare the optimization using vocabulary expansion,if you have not yet seen an inverted index search implementation, this implementation below is commented [here](https://github.com/IonesioJunior/Information-Retrieval/tree/master/search_by_inverted_index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "match_words = {}\n",
    "\n",
    "\n",
    "def add_dictionary( info_tuple ):\n",
    "    '''\n",
    "        This method fill lists of news index with match words\n",
    "        Args:\n",
    "            info_tuple(String, Int) : this tuple contain a specific word and index of news\n",
    "    '''\n",
    "    word, index = info_tuple\n",
    "    try:\n",
    "        match_words[ word.lower() ].add(index)\n",
    "    except KeyError:\n",
    "        match_words[ word.lower() ] = set( [ index ] )\n",
    "\n",
    "\n",
    "def extract_words( text , id_col ):\n",
    "    ''' \n",
    "        This method extract match words from all text tables and store in a dictionary correlating with news index\n",
    "        \n",
    "        Args:\n",
    "                text() : table with text to extract words\n",
    "                id_col : table with news index\n",
    "    '''\n",
    "    [  list(map(add_dictionary,[ ( word, id_col[i] ) for word in text[i] ] )) for i in range( len(text) ) ]\n",
    "\n",
    "\n",
    "\n",
    "def search( words ):\n",
    "    '''\n",
    "        This method search a set of words using AND/OR operators to guide how to search\n",
    "        Args:\n",
    "                words(String) : string with words to be searched (Ex: word1 AND word2 AND ... | word1 OR word2 OR ... )\n",
    "        Return:\n",
    "                index_list[Int] : list of news index that matches with words\n",
    "    '''\n",
    "    if ( \"AND\" in words ):\n",
    "        list_of_words = list(map( lambda x: x.strip(), words.split(\"AND\") ))\n",
    "        return reduce( lambda x,y : list( x & y ) , [ match_words[word.lower()] for word in list_of_words ] )\n",
    "    elif (\"OR\" in words ):\n",
    "        list_of_words = list(map(lambda x: x.strip(), words.split(\"OR\")))\n",
    "        return reduce( lambda x,y : list( set(x) | set(y) ) , [ match_words[word.lower()] for word in list_of_words ] )\n",
    "    else:\n",
    "        return list( match_words[words] )\n",
    "\n",
    "# READING AND PROCESSING DOCUMENTS\n",
    "dataframe = pd.read_csv(\"estadao_noticias_eleicao.csv\", encoding=\"utf-8\")\n",
    "file_csv = dataframe.titulo + \" \" + dataframe.subTitulo + \" \" + dataframe.conteudo\n",
    "file_csv = file_csv.fillna(\"\")\n",
    "\n",
    "#FILTERING DATASET\n",
    "# Remove dots\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens_lists = file_csv.apply(lambda text: tokenizer.tokenize(text.lower()))\n",
    "\n",
    "#Remove portuguese stop words\n",
    "stopword_ = stopwords.words('portuguese')\n",
    "filtered_tokens = tokens_lists.apply(lambda tokens: [token for token in tokens if token not in stopword_])\n",
    "\n",
    "#BUILD CORPUS\n",
    "extract_words(filtered_tokens,dataframe.idNoticia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Expanding results using co-ocurrency( Third step )</h3>\n",
    "Now, we will expand our results using top-3 co-occurrency words (using OR search operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_expanding_search(word):\n",
    "    result = get_high_words_co_ocurrency(word)\n",
    "    result.append(word)\n",
    "    return search( \" OR \".join(result) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Expansive search analysis</h1>\n",
    "After completing the implementation of structures, we will analyze the effects of these modifications in relation to the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Empirical Test</h4>\n",
    "For an empirical test, we chose the name of three famous people in the Brazilian political scene: **dilma**, **aécio** and **lula**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words associated with dilma: ['rousseff', 'é', 'disse']\n",
      "Most common words associated with lula: ['silva', 'dilma', 'disse']\n",
      "Most common words associated with aécio: ['neves', 'disse', 'é']\n"
     ]
    }
   ],
   "source": [
    "print (\"Most common words associated with dilma: \" + str(get_high_words_co_ocurrency(\"dilma\")) )\n",
    "print (\"Most common words associated with lula: \" + str(get_high_words_co_ocurrency(\"lula\")) )\n",
    "print (\"Most common words associated with aécio: \" + str(get_high_words_co_ocurrency(\"aécio\")) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Results</h4>\n",
    "-As expected, in all queries, we have a very strong relationship between the first name (searched) and the second name (obtained).\n",
    "\n",
    "-We can also observe a strong relation between two of the people submitted in the search: Lula and Dilma. A proven relationship if we look at the Brazilian political scene.\n",
    "\n",
    "-We can also observe the presence of the word \"disse\" in all searches. This is quite consistent, given that most of the documents related to these people are linked to presidential elections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Quality of results</h4>\n",
    "Expansive search facilitates the obtaining of results related to the query.But, **more results do not guarantee better results**. Many related results may turn out to be noises that do not have much relation with the searched one, reducing query accuracy/precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Is query expansion better suited to improving recall or accuracy? Why?</h3>\n"
    "As mentioned earlier, expansive search increase number of results decreasing precision and increasing the recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
