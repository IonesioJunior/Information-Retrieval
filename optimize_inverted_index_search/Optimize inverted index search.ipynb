{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "\n",
    "import pandas\n",
    "import nltk\n",
    "import ast\n",
    "from math import log\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "__author__ = \"Ionesio Junior\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Documents</h2>\n",
    "First, let's build an abstraction of some document stored in our corpus. This document needs to store its index, the first word used to register in corpus and a dict of words that this document contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Documents(object):\n",
    "    def __init__(self, index, first_word, tf_first_word):\n",
    "        self.__index = index\n",
    "        self.__first_word = first_word\n",
    "        self.__dict_of_words = { first_word : tf_first_word }\n",
    "\n",
    "\n",
    "    def get_index(self):\n",
    "        return self.__index\n",
    "\n",
    "    def append_new_term(self, word, word_frequency):\n",
    "        self.__dict_of_words[word] = word_frequency\n",
    "\n",
    "    def get_term_frequency(self, word=None):\n",
    "        if word == None:\n",
    "            self.__dict_of_words[this.__first_word]\n",
    "        else:\n",
    "            try:\n",
    "                return self.__dict_of_words[word]\n",
    "            except KeyError:\n",
    "                return 0\n",
    "\n",
    "    def get_words_dict(self):\n",
    "        return self.__dict_of_words\n",
    "\n",
    "    def update(self, other_doc):\n",
    "        self.__dict_of_words.update(other_doc.get_words_dict())\n",
    "\n",
    "    def __eq__(self,other):\n",
    "        return self.__index == other.get_index()\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Corpus</h2>\n",
    "Now, we need build an abstraction of our corpus. This object will read some csv file mapping all info about documents in your dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, doc_path):\n",
    "        file_csv = pandas.read_csv(doc_path, encoding=\"utf-8\")\n",
    "        file_csv = file_csv.replace(np.nan, '',regex = True)\n",
    "        self.__corpus_length = len(file_csv.idNoticia)\n",
    "        text = file_csv.titulo + \" \" + \" \" +  file_csv.subTitulo + \" \" + file_csv.conteudo\n",
    "        text.apply(lambda x: \"\" if isinstance(x, float) else self.__text_clear(x).lower())\n",
    "        self.__match_words = {}\n",
    "        self.__k = 1.5\n",
    "        self.__extract_words(text,file_csv.idNoticia)\n",
    "\n",
    "    def __text_clear(self,text):\n",
    "        pattern = re.compile('[^a-zA-Z0-9 ]')\n",
    "        text = normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "        return pattern.sub(' ', text)\n",
    "    \n",
    "    \n",
    "    def __add_dictionary(self,info_tuple):\n",
    "        word, index, text = info_tuple\n",
    "        try:\n",
    "            self.__match_words[ word.lower() ].add( Documents(index, word.lower(), text.count(word)) )\n",
    "        except KeyError:\n",
    "            self.__match_words[ word.lower() ] = set( [ Documents(index, word.lower(), text.count(word))] )\n",
    "\n",
    "    def __extract_words(self,text , id_col):\n",
    "        [ map( self.__add_dictionary, [ ( word, id_col[i],text[i] ) for word in nltk.word_tokenize( text[i] ) ] ) for i in xrange( len(text) ) ]\n",
    "\n",
    "\n",
    "    def __idf(self, word):\n",
    "        return log( self.__corpus_length + 1 / len(self.__match_words[word]) )\n",
    "\n",
    "    def __bm25(self, term_frequency):\n",
    "        return ((self.__k + 1)* term_frequency) / (self.__k + term_frequency)\n",
    "\n",
    "    def __AND_between_documents(self,x,y):\n",
    "        new_list = []\n",
    "        for first_doc in x:\n",
    "            for second_doc in y:\n",
    "                if(second_doc == first_doc):\n",
    "                    first_doc.update(second_doc)\n",
    "                    new_list.append(first_doc)\n",
    "        return set(new_list) \n",
    "\n",
    "    def search(self,type_of_search, words):\n",
    "        if ( \" \" in words ):\n",
    "            list_of_words = list( set( map( lambda x: ( x.strip().lower(), words.count(x) ), words.split(\" \") ) ) )\n",
    "            results = reduce( lambda x,y : self.__AND_between_documents(x,y) , [ self.__match_words[word[0].lower()] for word in list_of_words ] )\n",
    "        else:\n",
    "            results = list( self.__match_words[words] )\n",
    "        if type_of_search.lower() == \"binary\":\n",
    "            return map(lambda x: x.get_index(), results)\n",
    "        elif type_of_search.lower() == \"tf\":\n",
    "            tf_results = [ ( result.get_index(), sum( map(lambda x: x[1] * result.get_words_dict()[x[0]], list_of_words )) ) for result in results ]\n",
    "            return sorted(tf_results, key=lambda x: x[1],reverse=True)\n",
    "        elif type_of_search.lower() == \"tf-idf\":\n",
    "            tf_idf_results = [ ( result.get_index(), sum( map(lambda x: x[1] * result.get_words_dict()[x[0]] * self.__idf(x[0]), list_of_words )) ) for result in results ]\n",
    "            return sorted(tf_idf_results, key=lambda x: x[1],reverse=True)\n",
    "        elif type_of_search.lower() == \"bm25\":\n",
    "            bm25_results = [ ( result.get_index(), sum( map(lambda x: x[1] * self.__bm25(result.get_words_dict()[x[0]]) * self.__idf(x[0]), list_of_words )) ) for result in results ]\n",
    "            return sorted(bm25_results, key=lambda x: x[1],reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Precision Analysis Methods</h2>\n",
    "Methods given by instructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    corpus = Corpus(\"/database/estadao_noticias_eleicao.csv\")\n",
    "    ground_truth = pandas.read_csv(\"database/gabarito.csv\", encoding=\"utf-8\")\n",
    "\n",
    "    #Ground Truth results\n",
    "    google_results = map(lambda x: ast.literal_eval(x), ground_truth.google)\n",
    "    binary_search_results = map(lambda x: ast.literal_eval(x), ground_truth.busca_binaria)\n",
    "    tf_results = map(lambda x: ast.literal_eval(x),ground_truth.tf)\n",
    "    idf_results = map(lambda x: ast.literal_eval(x),ground_truth.tfidf)\n",
    "    bm25_results = map(lambda x: ast.literal_eval(x),ground_truth.bm25)\n",
    "\n",
    "\n",
    "\n",
    "    # My algorithms results\n",
    "\n",
    "    #Binary Results\n",
    "    binary_results = [ corpus.search(\"binary\", word) for word in ground_truth.str_busca ]\n",
    "    print (\"Result for simple binary search:  %.4f\" % mapk(binary_search_results,binary_results, k=5))\n",
    "\n",
    "    #Term frequency Results\n",
    "    custom_tf_results = [ map( lambda x: x[0], corpus.search(\"tf\", word) ) for word in ground_truth.str_busca ]\n",
    "    print (\"Result for term frequency search: %.4f\" % mapk(tf_results,custom_tf_results,k=5))\n",
    "\n",
    "\n",
    "    #Term frequency / Inverted document frequency results\n",
    "    custom_tf_idf_results = [ map( lambda x: x[0], corpus.search(\"tf-idf\", word) ) for word in ground_truth.str_busca ]\n",
    "    print (\"Result for tf / idf search: %.4f\" % mapk(idf_results,custom_tf_idf_results,k=5))\n",
    "\n",
    "    #Term frequency / Inverted document frequency results\n",
    "    custom_bm25_results = [ map( lambda x: x[0], corpus.search(\"bm25\", word) ) for word in ground_truth.str_busca ]\n",
    "    print (\"Result for bm25 search: %.4f\" % mapk(idf_results,custom_bm25_results,k=5))\n",
    "\n",
    "    print \" \"\n",
    "    print \"Testing with Google results ...\"\n",
    "    print (\"Result for simple binary search:  %.4f\" % mapk(google_results,binary_results, k=5))\n",
    "    print (\"Result for term frequency search: %.4f\" % mapk(google_results,custom_tf_results,k=5))\n",
    "    print (\"Result for tf / idf search: %.4f\" % mapk(google_results,custom_tf_idf_results,k=5))\n",
    "    print (\"Result for bm25 search: %.4f\" % mapk(google_results,custom_bm25_results,k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for simple binary search:  0.7607\n",
      "Result for term frequency search: 0.8620\n",
      "Result for tf / idf search: 0.5220\n",
      "Result for bm25 search: 0.5420\n",
      " \n",
      "Testing with Google results ...\n",
      "Result for simple binary search:  0.0000\n",
      "Result for term frequency search: 0.0400\n",
      "Result for tf / idf search: 0.0400\n",
      "Result for bm25 search: 0.0600\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
