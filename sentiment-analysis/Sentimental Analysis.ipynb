{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sentimental Analysis using Twitter messages</h1>\n",
    "This notebook tries to explain step-by-step about the development of an algorithm that tries to classify the sentimental characteristics of some phrase using NaiveBayes concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Ion√©sio Junior\"\n",
    "\n",
    "import pandas as pd\n",
    "import nltk, re\n",
    "from string import punctuation as punct\n",
    "from collections import OrderedDict\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier \n",
    "from nltk.classify.util import accuracy as nltk_acc\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data pre-processing</h2>\n",
    "This snippet implements some functions to load and filter our data set.We need to remove links/hashtags and punctuation of our data before train some model.After that, we'll get token words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def filter_by_stopwords(word):\n",
    "    if word not in stopwords and word not in punct:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_dataset(data_text):\n",
    "    # Remove URLS / Hashtags / links\n",
    "    data_text = re.sub(r'@\\S+', '', data_text)\n",
    "    data_text = re.sub(r'http\\S+', '', data_text)\n",
    "    data_text = re.sub(r'#\\S+', '', data_text)\n",
    "\n",
    "    # Filter stop words and extract tokens\n",
    "    tokens = list( filter( lambda word: filter_by_stopwords(word), nltk.word_tokenize( data_text.lower() ) ))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Structuring our data set</h2>\n",
    "Now, we need to put and organize our data set in a \"bag of words\" structure and label the bags.After that, we'll separate dataset by label (pos / neg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag_of_words(tweet_text):\n",
    "    ''' \n",
    "        Construct an abstraction of concept \"bag of words\" to each tweet\n",
    "        Args:\n",
    "            Tweet_text(String) : text of tweet message\n",
    "        Return:\n",
    "            {Word:Boolean} : Bag of words\n",
    "    '''\n",
    "    return { word:True for word in filter_dataset(tweet_text) }\n",
    "\n",
    "def extract_labels(dataset):\n",
    "    '''\n",
    "        Extract labels and filter dataset\n",
    "        \n",
    "        Params:\n",
    "            DataSet(DataFrame) : Set of tweets previously labeled\n",
    "        Return:\n",
    "            (RotuloPositivo, RotuloNegativo) : Separated/filtered labels \n",
    "    '''\n",
    "    positive_label = dataset[dataset.sentiment == 1].text\n",
    "    negative_label = dataset[dataset.sentiment == 0].text\n",
    "    filtered_positive_label = [ (build_bag_of_words(tweet),\"pos\") for tweet in positive_label ]\n",
    "    filtered_negative_label = [ (build_bag_of_words(tweet),\"neg\") for tweet in negative_label ]\n",
    "    return (filtered_positive_label, filtered_negative_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>\n",
    "After filtering,structuring and labeling our dataset, we can train our classifier. But, for test principles we'll divide our data (70% to train and 30% to test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset):\n",
    "    '''\n",
    "        Build and train a model of classfier\n",
    "        \n",
    "        Args:\n",
    "            DataSet(DataFrame) : data set to be used by classifier\n",
    "        Return:\n",
    "            classifier : trained classfier\n",
    "    '''\n",
    "    # Extracting filtered and labeled text data\n",
    "    positive_label, negative_label = extract_labels( dataset )\n",
    "    \n",
    "    # Separing data set (70% train / 30% test)\n",
    "    dataset_size = len(positive_label)\n",
    "    train_set = positive_label[:floor(dataset_size * 0.7)] + negative_label[:floor(dataset_size * 0.7)]\n",
    "    test_set = positive_label[floor(dataset_size * 0.7):] + negative_label[floor(dataset_size * 0.7):]\n",
    "    \n",
    "    # Training our model\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = pd.read_csv('database/db.csv',encoding='utf-8', sep='\\t')\n",
    "    classifier = train_model(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
